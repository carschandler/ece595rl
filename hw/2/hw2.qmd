---
title: "ECE59500RL HW2"
author: "Robert (Cars) Chandler â€” chandl71@purdue.edu"
format:
  pdf
jupyter: python3
---

## Problem 1

## Problem 2

## Problem 3

## Problem 4

First, we need to assign labels to the states, which are the different spaces on
the board. We will use a zero-indexed $x$-$y$ coordinate system to refer to the
different states $s_{xy} \in \mathcal{S}$, with the origin at the bottom left
square, $s_{0,0}$. Moving horizontally will increase the $x$-component and
vertically the $y$-component, so that our state space is 

$$
\mathcal{S} =\{s_{ij} : i, j \in \mathbb{N}_{0}, \quad i,j \le 5 \}
$$

::: {.callout-note}
### Note

Although we are using two "dimensions" to identify each state, we
still treat it as a one-dimensional vector, so that we have one row for each $s
\in \mathcal{S}$ in $\vec{v}$, $P^{\pi}$, and so forth. The order of the states
for this vector will always be in row-major order:

$$
(0, 0), (1, 0), (2, 0), (3, 0), (4, 0), (0, 1), (1, 1), ..., (3, 4), (4, 4)
$$
:::

### 4.1.a

The policy can be evaluated analytically using the following equation:

$$
\vec{v}^{\pi} = (I - \gamma P^{\pi})^{-1} \vec{R}^{\pi}
$$ {#eq-analytical-soln}

We have $\gamma = 0.95$ from the problem statement. $P^{\pi}$ and
$\vec{R}^{\pi}$ each need to be evaluated by going over each state in
$\mathcal{S}$ and using the information given to us to evaluate them. Beginning
with $P^{\pi}$:

$$
P^{\pi}_{ij} = P(s_{j} | s_{i}, a) = P(s_{j} | s_{i}, \pi(s_{i}))
$$

We can use Python to encode the logic described in the problem statement to
programatically calculate $P^{\pi}$ for each state transition:

```{python}
from enum import Enum
import numpy as np
import itertools
from IPython.display import Markdown


class Space(Enum):
    LIGHTNING = -1
    NORMAL = 0
    MOUNTAIN = 1
    TREASURE = 2


width = 5

board = np.full([width, width], Space.NORMAL)

board[2, 1] = Space.MOUNTAIN
board[3, 1] = Space.MOUNTAIN
board[1, 3] = Space.MOUNTAIN
board[2, 3] = Space.LIGHTNING
board[4, 4] = Space.TREASURE

policy = np.array(
    [
        list("URRUU"),
        list("UDDDU"),
        list("UURRR"),
        list("LULLU"),
        list("RRRRU"),
    ]
).T

p = np.zeros([25, 25])


def i_1d(x, y):
    return np.ravel_multi_index([y, x], dims=[width, width])


def is_blocked(x, y):
    return (
        x < 0
        or y < 0
        or x >= width
        or y >= width
        or board[x, y] == Space.MOUNTAIN
    )


for x, y in itertools.product(range(5), range(5)):
    i_cur_1d = i_1d(x, y)

    if board[x, y] != Space.NORMAL:
        p[i_cur_1d, i_cur_1d] = 1
        continue

    for a, (i2, j2) in zip(
        list("LRUD"), [[x - 1, y], [x + 1, y], [x, y + 1], [x, y - 1]]
    ):
        prob = 0.85 if a == policy[x, y] else 0.05
        if is_blocked(i2, j2):
            p[i_cur_1d, i_cur_1d] += prob
        else:
            p[i_cur_1d, i_1d(i2, j2)] += prob

state_text = []
for i in range(width**2):
    y1, x1 = np.unravel_index(i, [width, width])
    a = policy[x1, y1]
    for j in np.argwhere(p[i]).flatten():
        y2, x2 = np.unravel_index(j, [width, width])
        state_text.append(
            rf"P^{{\pi}}(s_{{ {x2}, {y2} }} | s_{{ {x1}, {y1} }}, \pi(s_{{"
            rf" {x1},"
            rf" {y1} }}) = \text{{{a}}}) &= {p[i, j]:g} \\"
        )
    state_text.append(r"\\")

state_text = "\n".join(state_text)


def vecfmt(v):
    return Markdown(", ".join([f"{e:g}" for e in v]))
```

The resulting state transition probabilities are listed as follows:

\begingroup
\allowdisplaybreaks
\begin{align*}
`{python} Markdown(state_text)`
\end{align*}
\endgroup

All other possible state transitions have probability $0$.

Moving onto $\vec{R}^{\pi}$:

$$
\vec{R}^{\pi} = \begin{bmatrix}
R(s_{1,1}, \pi(s_{1,1}) \\
R(s_{1,2}, \pi(s_{1,2}) \\
\cdots \\
R(s_{4,4}, \pi(s_{4,4}))
\end{bmatrix}_{|\mathcal{S}| \times 1}
$$

So given the reward function described, we just have a simple vector with two
nonzero elements:

```{python}
r = np.zeros(width * width, dtype=int)
r[i_1d(*np.argwhere(board == Space.LIGHTNING).squeeze())] = -1
r[i_1d(*np.argwhere(board == Space.TREASURE).squeeze())] = 1
```

$$
\vec{R}^{\pi} = \left[ `{python} vecfmt(r)` \right]^T
$$

So, now we can substitute in our values into @{eq-analytical-soln} and solve:

```{python}
gamma = 0.95
v = np.linalg.inv(np.identity(width**2) - gamma * p) @ r
value_text = []
for i, val in enumerate(v):
    y, x = np.unravel_index(i, [width, width])
    value_text.append(rf"v^{{\pi}}(s_{{ {x},{y} }}) &= {val:g} \\")

value_text = "\n".join(value_text)
```

\begingroup
\allowdisplaybreaks
\begin{align*}
`{python} Markdown(value_text)`
\end{align*}
\endgroup

### 4.1.b

Let $\vec{v}_0 = \mathbf{0}$. Then for each step in the iteration,

$$
\vec{v}_{t+1} = \vec{R}^{\pi} + \gamma P^{\pi} \vec{v}_{t}
$$

To determine $T$, the number of iterations we need to make in order to obtain
$\Vert v_T - v^\pi \Vert_{\infty} \le 0.01$, we can use the following theorem:

$$
T \ge \frac{\log \left( \frac{\Vert \vec{v}_{0} - \vec{v}^{\pi}\Vert_{\infty} }{\varepsilon} \right)}{\log \frac{1}{\gamma}}
$$

Which we can solve using Python, and then perform $T$ iterations by implementing
the equation above:

```{python}
epsilon = 0.01
v0 = np.zeros(width**2)
n_timesteps = int(
    np.ceil(np.log(np.max(v0 - v) / epsilon) / np.log(1 / gamma))
)

v_t = v0
v_history = [v0]
for t in range(n_timesteps):
    v_t = r + gamma * p @ v_t
    v_history.append(v_t)

v_history = np.array(v_history)

value_text_iter = []
for i, val in enumerate(v):
    y, x = np.unravel_index(i, [width, width])
    value_text_iter.append(
        rf"v_{{T = {n_timesteps}}} (s_{{ {x},{y} }}) &= {val:g} \\"
    )

value_text_iter = "\n".join(value_text_iter)
```

We perform $T = `{python} n_timesteps`$ iterations, and our final $v_T$ is:

\begingroup
\allowdisplaybreaks
\begin{align*}
`{python} Markdown(value_text_iter)`
\end{align*}
\endgroup

We can verify that our desired conditon holds:

```{python}
max_error = np.max(v_t - v)
```

So,

$$
\Vert v_T - v^\pi \Vert_{\infty} = `{python} Markdown(f"{max_error:0.6f}")` \le 0.01
$$

And if we were to have performed just one less iteration, this value would
be above the desired $\varepsilon = 0.01$.

### 4.1.c

We kept track of the full history of $v_t$, so we can calculate the error for
each timestep and plot it:

```{python}
import plotly.graph_objects as go
import plotly.io as pio

pio.renderers.default = "png"
pio.kaleido.scope.default_scale = 2

error_history = np.max(v_history - v, axis=1)
go.Figure(
    data=[go.Scatter(y=error_history, mode="lines")],
    layout=dict(
        xaxis_title="$t$", yaxis_title=r"$\Vert v_t - v^\pi \Vert_{\infty}$"
    ),
)
```

## 4.2

We can use use the following theorem to obtain the number of iterations
required: for an accuracy level of $\varepsilon$ in estimating the optimal value
function, we can run the value iteration algorithm for $T$ iterations such that:

$$
T \ge \frac{\log \left( \frac{\Vert \vec{v}_{0} - \vec{v}^{*}\Vert_{\infty} }{\varepsilon} \right)}{\log \frac{1}{\gamma}}
$$

which ensures that $\Vert v_T - v^* \Vert_{\infty} \le \varepsilon$

```{python}
v_0 = np.zeros(width**2)

n_timesteps = np.log(np.max(v_0 - ))

```
