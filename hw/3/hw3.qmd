---
title: "ECE59500RL HW3"
author: "Robert (Cars) Chandler â€” chandl71@purdue.edu"
# latex-tinytex: false
format:
  pdf:
    monofont: "CommitMono Nerd Font"
jupyter: python3
---

## Problem 1
### 1.1
Using the definition of the state-action occupancy measure under a stationary, stochastic policy:

$$
\nu_{\mu_{0}}^{\pi}(s,a) = \sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s, A_{t}=a | \mu_{0}, \pi, P) 
$$

The state-action occupancy measure takes a state and action pair and calculates the total (discounted) probability that the state and action at any time are that state and action. If we sum this across all actions, then we are really just finding the discounted probability that the state is that current state for all timesteps.

What we are saying below is the probability of the first state being the state in the parameter plus the sum of the probability of transitioning from some state-action combo to the state in question times the total probability across all steps of that state-action combo, across all state-action combos.

$$
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a)  =
\mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s',a')
$$

If we sum over all actions:

$$
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) = \sum_{a \in \mathcal{A}} \sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s, A_{t}=a | \mu_{0}, \pi, P) 
$$

Then we can drop the $A_{t} = a$ part since sum of the probability of a state-action pair across all possible actions will marginalize out the action on the probability measurement. From there, we can pull out the first term in the sum, marginalize the resulting probability inside the sum across all possible states and actions of the previous timestep, rearrange our sums, and realize that we have another state-action occupancy measure defined:

$$
\begin{aligned}
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) &= \sum_{a \in \mathcal{A}} \sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s, A_{t}=a | \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t}\mathbb{P}(S_{t}=s | \mu_{0}, \pi, P) \\
&= \gamma^{0} \mathbb{P} (S_{0} = s | \mu_{0}, \pi, P) + \sum_{t=1}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s | \mu_{0}, \pi, P) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s | \mu_{0}, \pi, P) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t} \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \mathbb{P}(S_{t}=s, S_{t-1} = s', A_{t-1} = a' | \mu_{0}, \pi, P) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t} \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \mathbb{P}(S_{t}=s | S_{t-1} = s', A_{t-1} = a',  \mu_{0}, \pi, P)  \cdot \mathbb{P}\left( S_{t-1}=s', A_{t-1}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t} \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \mathbb{P}\left( S_{t-1}=s', A_{t-1}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \sum_{t=1}^{\infty}\gamma^{t} \mathbb{P}\left( S_{t-1}=s', A_{t-1}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \sum_{t=0}^{\infty}\gamma^{t+1} \mathbb{P}\left( S_{t}=s', A_{t}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \gamma\sum_{t=0}^{\infty}\gamma^{t} \mathbb{P}\left( S_{t}=s', A_{t}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \gamma \nu^{\pi}_{\mu_{0}}(s', a') \\
&= \mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s', a') & \blacksquare \\
\end{aligned}
$$

### 1.2
Given the definition of the state occupancy measure:

$$
\rho_{\mu_{0}}^{\pi}(s) = \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}(S_{t} = s| \mu_{0}, \pi, P)
$$

We can marginalize the probability over all actions in $\mathcal{A}$ and rearrange the order of the summation:

$$
\begin{aligned}
\rho_{\mu_{0}}^{\pi}(s) &= \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}(S_{t} = s| \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}(S_{t} = s| \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t} \sum_{a \in \mathcal{A}} \mathbb{P}(S_{t} = s, A_{t} = a| \mu_{0}, \pi, P) \\
&= \sum_{a \in \mathcal{A}} \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s, A_{t} = a| \mu_{0}, \pi, P) \\
\rho_{\mu_{0}}^{\pi}(s) &= \sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) & \blacksquare \\
\end{aligned}
$$

Given the definition of the state-action occupancy measure, we can use the definition of conditional probability to construct a product of two probabilities, substitute in the policy function using its definition, and finally use the definition of the state occupancy measure:

$$
\begin{aligned}
\nu_{\mu_{0}}^{\pi}(s,a) &= \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s, A_{t} = a| \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s | \mu_{0}, \pi, P) \mathbb{P}(A_{t} = a | S_{t} = s, \mu_{0}, \pi, P) \\ 
&= \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s | \mu_{0}, \pi, P) \pi(a|s) \\ 
\nu_{\mu_{0}}^{\pi}(s,a) &= \rho_{\mu_{0}}^{\pi}(s) \pi(a|s) & \blacksquare \\ 
\end{aligned}
$$

### 1.3
We just showed in [1.2] that
$$
\rho_{\mu_{0}}^{\pi}(s) = \sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a)
$$
And in [1.1] that

$$
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) = \mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s', a')
$$

So by the transitive property:

$$
\rho_{\mu_{0}}^{\pi}(s) = \mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s', a')
$$

To convert this into vector form, the first $\mu_{0}$ term in the sum is easy to take care of as it is added separately, so we can just form the equivalent vector:

$$
\mathbf{\mu_{0}} = \begin{bmatrix}
\mu_{0}(s_{1})  \\
\mu_{0}(s_{2})  \\
\vdots \\
\mu_{0}(s_{\lvert \mathcal{S} \rvert })  \\
\end{bmatrix}
$$

Next, we need the sum across all states and actions of the product of the transition function with the state-action occupancy measure. However, the transition function for each iteration of the summation needs to be evaluated for the state-action pair given by the summation transitioning to the state corresponding to the current index in the resulting vector. The state-action occupancy measure needs to be evaluated for the current state-action pair given by the summation. We also showed in [1.2] that

$$
\nu_{\mu_{0}}^{\pi}(s,a) = \rho_{\mu_{0}}^{\pi}(s) \pi(a|s)
$$

So the summation term in question can be rewritten as:

$$
\gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \rho_{\mu_{0}}^{\pi}(s') \pi(a'|s')
$$

If we rewrite the $P$ and $\pi$ terms in terms of the probabilities they express and expand these probabilities using the definition of conditional probability, we get:

$$
\begin{aligned}
&\gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \mathbb{P}(S_{t}=s | S_{t-1}=s', A_{t-1}=a') \mathbb{P}(A_{t-1} = a' | S_{t-1}=s') \rho_{\mu_{0}}^{\pi}(s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s', A_{t-1}=a')}{\mathbb{P}(S_{t-1}=s', A_{t-1}=a')}  \frac{\mathbb{P}(A_{t-1} = a' , S_{t-1}=s')}{\mathbb{P}(S_{t-1}=s')} \rho_{\mu_{0}}^{\pi}(s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s', A_{t-1}=a')}{\mathbb{P}(S_{t-1}=s')} \rho_{\mu_{0}}^{\pi}(s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') \sum_{a' \in \mathcal{A}} \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s', A_{t-1}=a')}{\mathbb{P}(S_{t-1}=s')} \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s')}{\mathbb{P}(S_{t-1}=s')} \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') \mathbb{P}(S_{t}=s | S_{t-1}=s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') P(s | s') \\
\end{aligned}
$$

And we can create this sum using matrix multiplication. Since $\mathbf{P}^{\pi}_{ij} = P(s_{j} | s_{i})$, this means that a single column of $\mathbf{P}^{\pi}$ corresponds to the different states $s'$ could be. Standard matrix multiplication of $\mathbf{P}^{\pi}$ against some column vector would multiply rows against the vector, though. So, if we want to multiply the columns to achieve a summation of the product of $P(s|s')$ with $\rho_{\mu_{0}}^{\pi}(s')$ across $s' \in \mathcal{S}$ rather than $s \in \mathcal{S}$, we need to transpose $\mathbf{P}^{\pi}$ first and then matrix-multiply it against $\mathbf{\rho_{\mu_{0}}}^{\pi}$:

$$
\begin{aligned}
\gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') P(s | s') &=  \gamma \begin{bmatrix}
P^{\pi}(s_{1} | s_{1}) & P^{\pi}(s_{2} | s_{1}) & \cdots & P^{\pi}(s_{\lvert \mathcal{S} \rvert } | s_{1}) \\
P^{\pi}(s_{1} | s_{2}) & P^{\pi}(s_{2} | s_{2}) & \cdots & P^{\pi}(s_{\lvert \mathcal{S} \rvert } | s_{2}) \\
\vdots & \vdots & \ddots & \vdots \\
P^{\pi}(s_{1} | s_{|\mathcal{S}|}) & P^{\pi}(s_{2} | s_{|\mathcal{S}|}) & \cdots & P^{\pi}(s_{\lvert \mathcal{S} \rvert } | s_{|\mathcal{S}|}) \\
\end{bmatrix}^{T}
\begin{bmatrix}
\rho^{\pi}_{\mu_{0}}(s_{1}) \\
\rho^{\pi}_{\mu_{0}}(s_{2}) \\
\vdots \\
\rho^{\pi}_{\mu_{0}}(s_{\lvert \mathcal{S} \rvert }) \\
\end{bmatrix} \\
&= \gamma {\mathbf{P}^{\pi}}^{T} \mathbf{\rho^{\pi}_{\mu_{0}}}
\end{aligned}
$$

So finally, substituting this into the original sum:

$$
\mathbf{\rho_{\mu_{0}}^{\pi}} = \mathbf{\mu_{0} } + \gamma {\mathbf{P}^{\pi}}^{T} \mathbf{\rho^{\pi}_{\mu_{0}}}
$$

### 1.4

$$
\begin{aligned}
\mathbf{\rho_{\mu_{0}}^{\pi}} &= \mathbf{\mu_{0} } + \gamma {\mathbf{P}^{\pi}}^{T} \mathbf{\rho^{\pi}_{\mu_{0}}}  \\
\mathbf{\rho_{\mu_{0}}^{\pi}} - \gamma {\mathbf{P}^{\pi}}^{T} \mathbf{\rho^{\pi}_{\mu_{0}}} &= \mathbf{\mu_{0} } \\
\left( \mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T} \right) \mathbf{\rho_{\mu_{0}}^{\pi}}  &= \mathbf{\mu_{0} } \\
\mathbf{\rho_{\mu_{0}}^{\pi}}  &=  \left( \mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T} \right)^{-1} \mathbf{\mu_{0} } \\
\end{aligned}
$$

The inverted quantity in the final line takes on the same form as the quantity proven to be invertible in Problem 1 of Homework 2: $\mathbf{I} - \gamma \mathbf{P}^{\pi}$. As a reminder, this quantity was shown to be invertible by an analysis of the eigenvalues. We showed that all the eigenvalues of $\mathbf{P}^{\pi}$ were nonzero by considering a theoretical eigenvalue to be greater than $1$, which then proved to be impossible. From there, we showed that the eigenvalues of the entire quantity would necessarily be nonzero, proving that the quantity was invertible.

Now, if $(\mathbf{I} - \gamma \mathbf{P}^{\pi})$ is invertible, then so is $(\mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T})$ since a matrix and its transpose always have the same eigenvalues, so the same argument applies to prove that this quantity is invertible.

To say that $(\mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T})$ is invertible is to say that the equation $\mathbf{x} (\mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T}) = \mathbf{b}$ has a unique for $\mathbf{x}$ given some arbitrary $\mathbf{b}$. To show this, consider some matrix $\mathbf{A}$ that is invertible:

$$
\begin{aligned}
\mathbf{Ax} &= \mathbf{b}  \\
\mathbf{A}^{-1} \mathbf{Ax} &= \mathbf{A}^{-1} \mathbf{b} \\
\mathbf{Ix} &= \mathbf{A}^{-1} \mathbf{b} \\
\mathbf{x} &= \mathbf{A}^{-1} \mathbf{b} \\
\end{aligned}
$$

$\mathbf{A}^{-1}\mathbf{b}$ always yields a single vector, which is the unique solution to the equation. Our equation 

$$
\left( \mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T} \right) \mathbf{\rho_{\mu_{0}}^{\pi}} = \mathbf{\mu_{0} }
$$

takes on this exact form, such that

$$
\mathbf{\rho_{\mu_{0}}^{\pi}} =  \left( \mathbf{I} - \gamma {\mathbf{P}^{\pi}}^{T} \right)^{-1} \mathbf{\mu_{0} }
$$

is the unique solution to $\rho_{\mu_{0}}^{\pi}$.

### 1.5
Using the definition of $\nu_{\mu_{0}}^{\pi}$ from [1.2], we can rearrange to solve for $\pi$, then substitute in the definition of $\rho_{\mu_{0}}^{\pi}(s)$ from [1.2]:

$$
\begin{aligned}
\nu_{\mu_{0}}^{\pi}(s,a) &= \rho_{\mu_{0}}^{\pi}(s) \pi(a|s) \\
\pi(a|s)  &= \frac{\nu_{\mu_{0}}^{\pi}(s,a) }{\rho_{\mu_{0}}^{\pi}(s)} \\
\pi(a|s)  &= \frac{\nu_{\mu_{0}}^{\pi}(s,a) }{\sum_{a' \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a')} \\
\end{aligned}
$$

This is the first part of the piecewise equation.

# TODO: is there more to do here?

## 1.6
It is given that

$$
V^{\pi}(\mu) = \mathbb{E}_{S \sim \mu}\left[ V^{\pi}(S) \right]  = \sum_{s \in \mathcal{S}} \mu(s) V^{\pi}(s)
$$

So, substituting in $\mu_{0}$:

$$
\begin{aligned}
V^{\pi}(\mu_{0}) &= \mathbb{E}_{S \sim \mu_{0}}\left[ V^{\pi}(S) \right]  = \sum_{s \in \mathcal{S}} \mu_{0}(s) V^{\pi}(s) \\
V^{\pi}(\mu_{0}) &= \sum_{s \in \mathcal{S}} \mu_{0}(s) \mathbb{E} \left[  \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right] \\
V^{\pi}(\mu_{0}) &= \sum_{s \in \mathcal{S}} \mathbb{P}(s_{0} = s) \sum_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} \left[ \mathbb{P}(s_{0} = s | s_{0} = s) \mathbb{P}(A_{0} = a | s_{0} = s) \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right] \\
V^{\pi}(\mu_{0}) &= \sum_{s \in \mathcal{S}} \mathbb{P}(s_{0} = s) \sum_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} \left[ 1 \cdot \pi(a|s) \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right] \\
\end{aligned}
$$

$$
v^{\pi}(s) = \mathbb{E} \left[
\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right], \quad \forall s \in
\mathcal{S}
$$


## Problem 2

### 2.1

If $\left| R(s,a) \right| \le R_{max}$ and

$$
G_{i}^{s} = \sum_{t=0}^{\left| \tau_{i}^{s} \right| } \gamma^{t}R(S^{s}_{t,i}, S^{s}_{t,i})
$$

then in the case where $R$ is greatest, i.e. $R = R_{max}$,

$$
G_{i}^{s} = \sum_{t=0}^{\left| \tau_{i}^{s} \right| } \gamma^{t} R_{max} = \frac{R_{max}}{1 - \gamma}
$$

and in the case where $R$ is least, i.e. $R=-R_{max}$,

$$
G_{i}^{s} = \sum_{t=0}^{\left| \tau_{i}^{s} \right| } -\gamma^{t} R_{max} = -\frac{R_{max}}{1 - \gamma}
$$

Therefore:

$$
- \frac{R_{max}}{1-\gamma} \le G_{i}^{s} \le \frac{R_{max}}{1 -\gamma}
$$

### 2.2

$$
E(s) = \sum_{i=1}^{N^{s}}G_{i}^{s}
$$

$$
\begin{aligned}
\mathbb{E}_{A_{t \sim \pi}, S_{t+1} \sim P}[E(s)] &= \mathbb{E}_{A_{t \sim \pi}, S_{t+1} \sim P} \left[ \sum_{i=1}^{N^{s}} G_{i}^{s} \right] \\
&= \sum_{i=1}^{N^{s}} \mathbb{E}_{A_{t \sim \pi}, S_{t+1} \sim P} \left[  G_{i}^{s} \right] \\
&= \sum_{i=1}^{N^{s}} V^{\pi}(s) \\
\end{aligned}
$$

### 2.3

# TODO: do we need to prove that the inequality holds with probability 1?

Since we were able to bound $G_{i}^{s}$ on both sides, and since $E(s)$ is a sum of that bound variable, we can apply Hoeffding's inequality to bound the probability that $E(s)$ deviates from its expected value. Let $s_{N^{s}} = E(s) = \sum_{i=1}^{N^{s}}G_{i}^{s}$ be the sum used in the inequality:

$$
\mathbb{P}\left( \left| E(s) - \mathbb{E}[E(s)] \right| \ge \varepsilon \right) \le 2 \exp \left( -\frac{2\varepsilon^{2}}{\sum_{i=1}^{N^{s}} \left( \frac{2R_{max}}{1-\gamma} \right)^{2} } \right) = 2 \exp \left( -\frac{\varepsilon^{2}}{\sum_{i=1}^{N^{s}} 2\left( \frac{R_{max}}{1-\gamma} \right)^{2} } \right)
$$

### 2.4
We can apply the corrolary to Hoeffding's inequality which allows us to bound the arithmetic mean of the summed term:

$$
\mathbb{P}\left(\left| \bar{s}_{t} - E[\bar{s}_{t}] \ge \varepsilon  \right| \right) \le 2 \exp \left( \frac{-2 \varepsilon^{2}t}{(b_{i} - a_{i})^{2}} \right), \quad \forall \varepsilon \ge 0
$$

If $s_{N^{s}}$ was the sum in the previous step, then 

$$
\bar{s}_{i} = \frac{1}{N^{s}}\sum_{i=1}^{N^{s}} G_{i}^{s} = \hat{V}^{\pi}(s)
$$

And then the expected value of $\bar{s}_{N^{s}}$ is:

$$
\mathbb{E} \left[ \frac{1}{N^{s}}\sum_{i=1}^{N^{s}} G_{i}^{s} \right] = \frac{1}{N^{s}}\sum_{i=1}^{N^{s}} \mathbb{E} \left[  G_{i}^{s} \right] = \frac{1}{N^{s}}\sum_{i=1}^{N^{s}} V^{\pi}(s) = \frac{1}{N^{s}} N^{s} V^{\pi}(s) = V^{\pi}(s)
$$

So we have $\bar{s}_{i}$ and $\mathbb{E}[\bar{s}_{i}]$ to use in the alternate form of Hoeffding's inequality:

$$
\mathbb{P}\left(\left| \hat{V}^{\pi}(s) - V^{\pi}(s) \right| \ge \varepsilon'\right) \le 2 \exp \left( -\frac{ 2(\varepsilon')^{2}N^{s}}{\left( \frac{2R_{max}}{1-\gamma} \right)^{2}} \right) = 2 \exp \left( -\frac{ (\varepsilon')^{2}N^{s}}{2 \left( \frac{R_{max}}{1-\gamma} \right)^{2}} \right), \quad \forall \varepsilon' \ge 0
$$

### 2.5
As $N^{s}$ decreases, the term inside the exponential in [2.4] becomes less negative, so the exponential term itself becomes greater. Therefore, the smallest $N^{s}$ will yield the largest $\mathbb{P}\left(\left| \hat{V}^{\pi}(s) - V^{\pi}(s) \right| \ge \varepsilon'\right)$. In other words, $N$ will yield the largest probability of all $N^{s}$, so we can use it to bound $\mathbb{P}\left( \lVert \hat{V}^{\pi} - V^{\pi} \rVert_{\infty} \ge \varepsilon'\right)$. We have already calculated the bound for the absolute difference any given state, and the L-$\infty$ norm will give us the maximum absolute difference across all states. We do not have a way to calculate which state will yield the maximum absolute difference, so we can instead calculate the total probability that the absolute difference will be greater than $\varepsilon'$ for *any* state, which is the union of all the events, which we can bound using Boole's inequality:

$$
\mathbb{P}\left( \lVert \hat{V}^{\pi} - V^{\pi} \rVert_{\infty} \ge \varepsilon'\right) \le \mathbb{P} \left( \bigcup_{s \in \mathcal{S}} \left| \hat{V}^{\pi}(s) - V^{\pi}(s) \right| \ge \varepsilon' \right) \le \sum_{s \in \mathcal{S}} \mathbb{P}\left(\left| \hat{V}^{\pi}(s) - V^{\pi}(s) \right| \ge \varepsilon' \right)
$$

And we can bound this sum using the bound we determined in [2.4] since the sum of a bounded function must be less than or equal to the sum of its bound. We substitute $N$ in for $N^{s}$ since we already showed how it will yield the greatest value, which is desired to achieve an upper bound:

$$
\mathbb{P}\left( \lVert \hat{V}^{\pi} - V^{\pi} \rVert_{\infty} \ge \varepsilon'\right) \le \sum_{s \in \mathcal{S}} \mathbb{P}\left(\left| \hat{V}^{\pi}(s) - V^{\pi}(s) \right| \ge \varepsilon' \right) \le \sum_{i=1}^{\left| \mathcal{S} \right| } 2 \exp \left( -\frac{ (\varepsilon')^{2}N}{2 \left( \frac{R_{max}}{1-\gamma} \right)^{2}} \right)
$$

$$
\sum_{i=1}^{\left| \mathcal{S} \right| } 2 \exp \left( -\frac{ (\varepsilon')^{2}N}{2 \left( \frac{R_{max}}{1-\gamma} \right)^{2}} \right) = 2 \left| S \right| \exp \left( -\frac{ (\varepsilon')^{2}N}{2 \left( \frac{R_{max}}{1-\gamma} \right)^{2}} \right)
$$

So finally:

$$
\mathbb{P}\left( \lVert \hat{V}^{\pi} - V^{\pi} \rVert_{\infty} \ge \varepsilon'\right) \le 
2 \left| S \right| \exp \left( -\frac{ (\varepsilon')^{2}N}{2 \left( \frac{R_{max}}{1-\gamma} \right)^{2}} \right)
, \quad \forall \varepsilon' > 0
$$


## Problem 3

First, we define the trajectories given, and after this we apply the every-visit
Monte Carlo method to estimate $Q^\pi(b, x)$ based on the collected sample
trajectories:

```{python}
import itertools
from enum import Enum, auto

import numpy as np
from numpy.typing import NDArray


# Declare spaces
class State(Enum):
    b = auto()
    c = auto()
    d = auto()


class Action(Enum):
    x = auto()
    y = auto()


class Trajectory:
    states: list[State]
    actions: list[Action]
    rewards: NDArray[np.int64]

    def __init__(self, states, actions, rewards) -> None:
        self.states = states
        self.actions = actions
        self.rewards = rewards


trajectories = [
    Trajectory(
        [State.d, State.b, State.d, State.c],
        [Action.x, Action.x, Action.y],
        np.array([1.5, -1, 2]),
    ),
    Trajectory(
        [State.b, State.d, State.b, State.b, State.d, State.d, State.c],
        [Action.x, Action.y, Action.y, Action.x, Action.x, Action.x],
        np.array([-1, 2, 0, -1, 1.5, 1.5]),
    ),
    Trajectory(
        [State.b, State.d, State.d, State.d, State.b, State.c],
        [Action.y, Action.x, Action.x, Action.y, Action.y],
        np.array([0, 1.5, 1.5, 2, 0]),
    ),
]

# Initialize vhat
nonterminal_states = [State.b, State.d]


def initilize_for_each_state_action_pair(val):
    return {
        (state, action): val
        for state, action in itertools.product(nonterminal_states, Action)
    }


qhat = initilize_for_each_state_action_pair(0.0)

# Initialize set of G
returns = initilize_for_each_state_action_pair([])

# Initialize N(s)
num_visits = initilize_for_each_state_action_pair(0)

for traj in trajectories:
    # For each state-action pair... (excluding state c since we end at that state)
    for state, action in itertools.product(nonterminal_states, Action):
        pair = (state, action)

        # If the pair in question isn't in the trajectory at all, then don't
        # attempt to update qhat
        if pair not in zip(traj.states[:-1], traj.actions):
            continue

        # For each step in the trajectory...
        for t, (s, a, r) in enumerate(
            zip(traj.states[:-1], traj.actions, traj.rewards)
        ):
            # Skip if the step does not match the state-action pair in question
            if s != state or a != action:
                continue

            # We aren't told about any discount, so the return is just the  sum
            # of the rewards from the current time forward
            ret = float(np.sum(traj.rewards[t:]))
            returns[pair].append(ret)
            num_visits[pair] += 1

        qhat[pair] = float((1 / num_visits[pair]) * np.sum(returns[pair]))

for k, v in qhat.items():
    print(f"Qhat({k[0].name}, {k[1].name}) = {v}")
```

The resulting $\hat{Q}^\pi$ is shown above, and we can see that

$$
Q^\pi(b, x) \approx \hat{Q}^\pi(b, x) = `{python} qhat[(State.b, Action.x)]`
$$

## Problem 4

To model the environment, we can re-use some of our code from HW2. It encodes
the board, the given policy, the state transition probabilities, and the reward
function:

```{python}
import itertools
from enum import Enum, StrEnum, auto

import numpy as np
from IPython.display import Markdown


class BoardSpace(StrEnum):
    NORMAL = "N"
    MOUNTAIN = "M"
    LIGHTNING = "L"
    TREASURE = "T"


class Action(StrEnum):
    UP = "U"
    RIGHT = "R"
    DOWN = "D"
    LEFT = "L"


width = 5

board = np.full([width, width], BoardSpace.NORMAL)

board[2, 1] = BoardSpace.MOUNTAIN
board[3, 1] = BoardSpace.MOUNTAIN
board[1, 3] = BoardSpace.MOUNTAIN
board[2, 3] = BoardSpace.LIGHTNING
board[4, 4] = BoardSpace.TREASURE

policy = np.flipud(
    np.array(
        [
            list("RRRRU"),
            list("LULLU"),
            list("UURRR"),
            list("UDDDU"),
            list("URRUU"),
        ]
    )
).T


def i_1d(x, y):
    return np.ravel_multi_index([y, x], dims=[width, width])


def is_blocked(x, y):
    return (
        x < 0
        or y < 0
        or x >= width
        or y >= width
        or board[x, y] == BoardSpace.MOUNTAIN
    )


def get_trans_prob(x: int, y: int, a: Action):
    if x < 0 or x >= width or y < 0 or y >= width:
        raise RuntimeError("Invalid coordinates")

    if a not in Action:
        raise RuntimeError("Invalid action type")

    p_vec = np.zeros(width**2)
    i_state_1d = i_1d(x, y)

    if board[x, y] != BoardSpace.NORMAL:
        p_vec[i_state_1d] = 1
        return p_vec

    direction_coordinates = {
        Action.LEFT: [x - 1, y],
        Action.RIGHT: [x + 1, y],
        Action.UP: [x, y + 1],
        Action.DOWN: [x, y - 1],
    }

    for direction, (x_next, y_next) in direction_coordinates.items():
        prob = 0.85 if direction == a else 0.05
        if is_blocked(x_next, y_next):
            p_vec[i_state_1d] += prob
        else:
            p_vec[i_1d(x_next, y_next)] += prob

    if p_vec.sum() != 1:
        raise RuntimeError("Probability vector did not add to 1")

    return p_vec


# (0, 0), (1, 0), (2, 0) ... (3, 4), (4, 4)
each_state = [(x, y) for y in range(5) for x in range(5)]

p = np.zeros([width**2, width**2])

for x, y in each_state:
    i = i_1d(x, y)
    p[i] = get_trans_prob(x, y, policy[x, y])

state_text = []
for i in range(width**2):
    y1, x1 = np.unravel_index(i, [width, width])
    a = policy[x1, y1]
    for j in np.argwhere(p[i]).flatten():
        y2, x2 = np.unravel_index(j, [width, width])
        state_text.append(
            rf"P^{{\pi}}(s_{{ {x2}, {y2} }} | s_{{ {x1}, {y1} }}, \pi(s_{{"
            rf" {x1},"
            rf" {y1} }}) = \text{{{a}}}) &= {p[i, j]:g} \\"
        )
    state_text.append(r"\\")

state_text = "\n".join(state_text)


def vecfmt(v):
    return Markdown(", ".join([f"{e:g}" for e in v]))
```

### 4.1

To implement the first-visit MC method, we need to ensure that each state is
visited often so its value is accurately estimated. In order to do this, we can
choose our initial state from a uniform random distribution across all states.
From here, we follow the policy until reaching a terminal state to generate our
sample episodic trajectory.

We can determine our termination criterion using Hoeffding's inequality

```{python}
r = np.zeros(width * width, dtype=int)
r[i_1d(*np.argwhere(board == BoardSpace.LIGHTNING).squeeze())] = -1
r[i_1d(*np.argwhere(board == BoardSpace.TREASURE).squeeze())] = 1


gamma = 0.95
v = np.linalg.inv(np.identity(width**2) - gamma * p) @ r
value_text = []
for i, val in enumerate(v):
    y, x = np.unravel_index(i, [width, width])
    value_text.append(rf"v^{{\pi}}(s_{{ {x},{y} }}) &= {val:g} \\")

value_text = "\n".join(value_text)

from numpy.linalg import norm

epsilon = 0.01
v0 = np.zeros(width**2)
n_iterations = int(
    np.ceil(np.log(1 / (epsilon * (1 - gamma))) / np.log(1 / gamma))
)

v_t = v0
v_history = [v0]
for t in range(n_iterations):
    v_t = r + gamma * p @ v_t
    v_history.append(v_t)

v_history = np.array(v_history)

value_text_iter = []
for i, val in enumerate(v_t):
    y, x = np.unravel_index(i, [width, width])
    value_text_iter.append(
        rf"v_{{T = {n_iterations}}} (s_{{ {x},{y} }}) &= {val:g} \\"
    )

value_text_iter = "\n".join(value_text_iter)


max_error = norm(v_t - v, ord=np.inf)


import plotly.graph_objects as go
import plotly.io as pio

pio.renderers.default = "png"
pio.kaleido.scope.default_scale = 2

error_history = norm(v_history - v, ord=np.inf, axis=1)
go.Figure(
    data=[go.Scatter(y=error_history, mode="lines")],
    layout=dict(
        xaxis_title="$t$", yaxis_title=r"$\Vert v_t - v^\pi \Vert_{\infty}$"
    ),
)


v0 = np.zeros(width**2)

v_t = v0

epsilon = 0.01

n_iterations = int(
    np.ceil(np.log(1 / (epsilon * (1 - gamma))) / np.log(1 / gamma))
)

for t in range(n_iterations):
    v_t = np.array(
        [
            np.max(
                [
                    r[i_1d(x, y)]
                    + gamma * np.sum(get_trans_prob(x, y, a) * v_t)
                    for a in Action
                ]
            )
            for x, y in each_state
        ]
    )

v_valiter = v_t


policy_opt = np.full_like(policy, "")

actions_list = list(Action)

for x, y in each_state:
    i_max = np.argmax(
        [
            r[i_1d(x, y)]
            + gamma * np.sum(get_trans_prob(x, y, a) * v_valiter)
            for a in actions_list
        ]
    )

    policy_opt[x, y] = actions_list[i_max].value

print(np.flipud(policy_opt.T))


p_opt = np.array(
    [get_trans_prob(x, y, policy_opt[x, y]) for x, y in each_state],
)

v_opt_valiter_policy = (
    np.linalg.inv(np.identity(width**2) - gamma * p_opt) @ r
)

value_text = []
for val, (x, y) in zip(v_opt_valiter_policy, each_state):
    value_text.append(rf"v^{{\pi_T}}(s_{{ {x},{y} }}) &= {val:g} \\")

value_text = "\n".join(value_text)


seed = 1298319824791827491284982176
rng = np.random.default_rng(seed)

int_to_action = np.vectorize(lambda i: actions_list[i])

policy_0 = int_to_action(rng.integers(low=0, high=4, size=(width, width)))

print(policy_0)


def evaluate_policy(policy):
    p_policy = np.array(
        [get_trans_prob(x, y, policy[x, y]) for x, y in each_state],
    )

    v_policy = np.linalg.inv(np.identity(width**2) - gamma * p_policy) @ r

    return v_policy


def improve_policy(policy, v_policy):
    for x, y in each_state:
        i_max = np.argmax(
            [
                r[i_1d(x, y)]
                + gamma * np.sum(get_trans_prob(x, y, a) * v_policy)
                for a in actions_list
            ]
        )
        policy[x, y] = actions_list[i_max]


policy_t = policy_0

v_policy_0 = evaluate_policy(policy_0)

n_iterations = int(
    np.ceil(
        np.log((norm(v_policy_0, np.inf) + (1 / (1 - gamma))) / epsilon)
        / np.log(1 / gamma)
    )
)

for i in range(n_iterations):
    v_policy_t = evaluate_policy(policy_t)

    improve_policy(policy_t, v_policy_t)

policy_opt = policy_t

print(np.flipud(policy_opt.T))


v_policy_opt = evaluate_policy(policy_opt)
value_text = []
for val, (x, y) in zip(v_policy_opt, each_state):
    value_text.append(rf"v^{{\pi_T}}(s_{{ {x},{y} }}) &= {val:g} \\")

value_text = "\n".join(value_text)
```
