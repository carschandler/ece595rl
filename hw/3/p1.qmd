## Problem 1

### 1.1

Using the definition of the state-action occupancy measure under a stationary, stochastic policy:

$$
\nu_{\mu_{0}}^{\pi}(s,a) = \sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s, A_{t}=a | \mu_{0}, \pi, P) 
$$

The state-action occupancy measure takes a state and action pair and calculates the total (discounted) probability that the state and action at any time are that state and action. If we sum this across all actions, then we are really just finding the discounted probability that the state is that current state for all timesteps.

What we are saying below is the probability of the first state being the state in the parameter plus the sum of the probability of transitioning from some state-action combo to the state in question times the total probability across all steps of that state-action combo, across all state-action combos.

$$
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a)  =
\mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s',a')
$$

If we sum over all actions:

$$
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) = \sum_{a \in \mathcal{A}} \sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s, A_{t}=a | \mu_{0}, \pi, P) 
$$

Then we can drop the $A_{t} = a$ part since sum of the probability of a state-action pair across all possible actions will marginalize out the action on the probability measurement. From there, we can pull out the first term in the sum, marginalize the resulting probability inside the sum across all possible states and actions of the previous timestep, rearrange our sums, and realize that we have another state-action occupancy measure defined:

$$
\begin{aligned}
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) &= \sum_{a \in \mathcal{A}} \sum_{t=0}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s, A_{t}=a | \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t}\mathbb{P}(S_{t}=s | \mu_{0}, \pi, P) \\
&= \gamma^{0} \mathbb{P} (S_{0} = s | \mu_{0}, \pi, P) + \sum_{t=1}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s | \mu_{0}, \pi, P) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t}\mathbb{P}(S_{t}=s | \mu_{0}, \pi, P) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t} \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \mathbb{P}(S_{t}=s, S_{t-1} = s', A_{t-1} = a' | \mu_{0}, \pi, P) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t} \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \mathbb{P}(S_{t}=s | S_{t-1} = s', A_{t-1} = a',  \mu_{0}, \pi, P)  \cdot \mathbb{P}\left( S_{t-1}=s', A_{t-1}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{t=1}^{\infty}\gamma^{t} \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \mathbb{P}\left( S_{t-1}=s', A_{t-1}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \sum_{t=1}^{\infty}\gamma^{t} \mathbb{P}\left( S_{t-1}=s', A_{t-1}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \sum_{t=0}^{\infty}\gamma^{t+1} \mathbb{P}\left( S_{t}=s', A_{t}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \gamma\sum_{t=0}^{\infty}\gamma^{t} \mathbb{P}\left( S_{t}=s', A_{t}=a' | \mu_{0}, \pi, P \right) \\
&= \mu_{0}(s) +  \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \cdot \gamma \nu^{\pi}_{\mu_{0}}(s', a') \\
&= \mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s', a') & \blacksquare \\
\end{aligned}
$$

### 1.2
Given the definition of the state occupancy measure:

$$
\rho_{\mu_{0}}^{\pi}(s) = \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}(S_{t} = s| \mu_{0}, \pi, P)
$$

We can marginalize the probability over all actions in $\mathcal{A}$ and rearrange the order of the summation:

$$
\begin{aligned}
\rho_{\mu_{0}}^{\pi}(s) &= \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}(S_{t} = s| \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t} \mathbb{P}(S_{t} = s| \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t} \sum_{a \in \mathcal{A}} \mathbb{P}(S_{t} = s, A_{t} = a| \mu_{0}, \pi, P) \\
&= \sum_{a \in \mathcal{A}} \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s, A_{t} = a| \mu_{0}, \pi, P) \\
\rho_{\mu_{0}}^{\pi}(s) &= \sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) & \blacksquare \\
\end{aligned}
$$

Given the definition of the state-action occupancy measure, we can use the definition of conditional probability to construct a product of two probabilities, substitute in the policy function using its definition, and finally use the definition of the state occupancy measure:

$$
\begin{aligned}
\nu_{\mu_{0}}^{\pi}(s,a) &= \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s, A_{t} = a| \mu_{0}, \pi, P) \\
&= \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s | \mu_{0}, \pi, P) \mathbb{P}(A_{t} = a | S_{t} = s, \mu_{0}, \pi, P) \\ 
&= \sum_{t=0}^{\infty} \gamma^{t}  \mathbb{P}(S_{t} = s | \mu_{0}, \pi, P) \pi(a|s) \\ 
\nu_{\mu_{0}}^{\pi}(s,a) &= \rho_{\mu_{0}}^{\pi}(s) \pi(a|s) & \blacksquare \\ 
\end{aligned}
$$

### 1.3
We just showed in [1.2] that
$$
\rho_{\mu_{0}}^{\pi}(s) = \sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a)
$$
And in [1.1] that

$$
\sum_{a \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a) = \mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s', a')
$$

So by the transitive property:

$$
\rho_{\mu_{0}}^{\pi}(s) = \mu_{0}(s) + \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \nu^{\pi}_{\mu_{0}}(s', a')
$$

To convert this into vector form, the first $\mu_{0}$ term in the sum is easy to take care of as it is added separately, so we can just form the equivalent vector:

$$
\boldsymbol{\mu_{0}} = \begin{bmatrix}
\mu_{0}(s_{1})  \\
\mu_{0}(s_{2})  \\
\vdots \\
\mu_{0}(s_{\lvert \mathcal{S} \rvert })  \\
\end{bmatrix}
$$

Next, we need the sum across all states and actions of the product of the transition function with the state-action occupancy measure. However, the transition function for each iteration of the summation needs to be evaluated for the state-action pair given by the summation transitioning to the state corresponding to the current index in the resulting vector. The state-action occupancy measure needs to be evaluated for the current state-action pair given by the summation. We also showed in [1.2] that

$$
\nu_{\mu_{0}}^{\pi}(s,a) = \rho_{\mu_{0}}^{\pi}(s) \pi(a|s)
$$

So the summation term in question can be rewritten as:

$$
\gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} P(s | s', a') \rho_{\mu_{0}}^{\pi}(s') \pi(a'|s')
$$

If we rewrite the $P$ and $\pi$ terms in terms of the probabilities they express and expand these probabilities using the definition of conditional probability, we get:

$$
\begin{aligned}
&\gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \mathbb{P}(S_{t}=s | S_{t-1}=s', A_{t-1}=a') \mathbb{P}(A_{t-1} = a' | S_{t-1}=s') \rho_{\mu_{0}}^{\pi}(s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s', A_{t-1}=a')}{\mathbb{P}(S_{t-1}=s', A_{t-1}=a')}  \frac{\mathbb{P}(A_{t-1} = a' , S_{t-1}=s')}{\mathbb{P}(S_{t-1}=s')} \rho_{\mu_{0}}^{\pi}(s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \sum_{a' \in \mathcal{A}} \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s', A_{t-1}=a')}{\mathbb{P}(S_{t-1}=s')} \rho_{\mu_{0}}^{\pi}(s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') \sum_{a' \in \mathcal{A}} \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s', A_{t-1}=a')}{\mathbb{P}(S_{t-1}=s')} \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') \frac{\mathbb{P}(S_{t}=s , S_{t-1}=s')}{\mathbb{P}(S_{t-1}=s')} \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') \mathbb{P}(S_{t}=s | S_{t-1}=s') \\
&= \gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') P(s | s') \\
\end{aligned}
$$

And we can create this sum using matrix multiplication. Since $\boldsymbol{P}^{\pi}_{ij} = P(s_{j} | s_{i})$, this means that a single column of $\boldsymbol{P}^{\pi}$ corresponds to the different states $s'$ could be. Standard matrix multiplication of $\boldsymbol{P}^{\pi}$ against some column vector would multiply rows against the vector, though. So, if we want to multiply the columns to achieve a summation of the product of $P(s|s')$ with $\rho_{\mu_{0}}^{\pi}(s')$ across $s' \in \mathcal{S}$ rather than $s \in \mathcal{S}$, we need to transpose $\boldsymbol{P}^{\pi}$ first and then matrix-multiply it against $\boldsymbol{\rho_{\mu_{0}}}^{\pi}$:

$$
\begin{aligned}
\gamma \sum_{s' \in \mathcal{S}} \rho_{\mu_{0}}^{\pi}(s') P(s | s') &=  \gamma \begin{bmatrix}
P^{\pi}(s_{1} | s_{1}) & P^{\pi}(s_{2} | s_{1}) & \cdots & P^{\pi}(s_{\lvert \mathcal{S} \rvert } | s_{1}) \\
P^{\pi}(s_{1} | s_{2}) & P^{\pi}(s_{2} | s_{2}) & \cdots & P^{\pi}(s_{\lvert \mathcal{S} \rvert } | s_{2}) \\
\vdots & \vdots & \ddots & \vdots \\
P^{\pi}(s_{1} | s_{|\mathcal{S}|}) & P^{\pi}(s_{2} | s_{|\mathcal{S}|}) & \cdots & P^{\pi}(s_{\lvert \mathcal{S} \rvert } | s_{|\mathcal{S}|}) \\
\end{bmatrix}^{T}
\begin{bmatrix}
\rho^{\pi}_{\mu_{0}}(s_{1}) \\
\rho^{\pi}_{\mu_{0}}(s_{2}) \\
\vdots \\
\rho^{\pi}_{\mu_{0}}(s_{\lvert \mathcal{S} \rvert }) \\
\end{bmatrix} \\
&= \gamma {\boldsymbol{P}^{\pi}}^{T} \boldsymbol{\rho^{\pi}_{\mu_{0}}}
\end{aligned}
$$

So finally, substituting this into the original sum:

$$
\boldsymbol{\rho_{\mu_{0}}^{\pi}} = \boldsymbol{\mu_{0} } + \gamma {\boldsymbol{P}^{\pi}}^{T} \boldsymbol{\rho^{\pi}_{\mu_{0}}}
$$

### 1.4

$$
\begin{aligned}
\boldsymbol{\rho_{\mu_{0}}^{\pi}} &= \boldsymbol{\mu_{0} } + \gamma {\boldsymbol{P}^{\pi}}^{T} \boldsymbol{\rho^{\pi}_{\mu_{0}}}  \\
\boldsymbol{\rho_{\mu_{0}}^{\pi}} - \gamma {\boldsymbol{P}^{\pi}}^{T} \boldsymbol{\rho^{\pi}_{\mu_{0}}} &= \boldsymbol{\mu_{0} } \\
\left( \boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T} \right) \boldsymbol{\rho_{\mu_{0}}^{\pi}}  &= \boldsymbol{\mu_{0} } \\
\boldsymbol{\rho_{\mu_{0}}^{\pi}}  &=  \left( \boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T} \right)^{-1} \boldsymbol{\mu_{0} } \\
\end{aligned}
$$

The inverted quantity in the final line takes on the same form as the quantity proven to be invertible in Problem 1 of Homework 2: $\boldsymbol{I} - \gamma \boldsymbol{P}^{\pi}$. As a reminder, this quantity was shown to be invertible by an analysis of the eigenvalues. We showed that all the eigenvalues of $\boldsymbol{P}^{\pi}$ were nonzero by considering a theoretical eigenvalue to be greater than $1$, which then proved to be impossible. From there, we showed that the eigenvalues of the entire quantity would necessarily be nonzero, proving that the quantity was invertible.

Now, if $(\boldsymbol{I} - \gamma \boldsymbol{P}^{\pi})$ is invertible, then so is $(\boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T})$ since a matrix and its transpose always have the same eigenvalues, so the same argument applies to prove that this quantity is invertible.

To say that $(\boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T})$ is invertible is to say that the equation $\boldsymbol{x} (\boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T}) = \boldsymbol{b}$ has a unique for $\boldsymbol{x}$ given some arbitrary $\boldsymbol{b}$. To show this, consider some matrix $\boldsymbol{A}$ that is invertible:

$$
\begin{aligned}
\boldsymbol{Ax} &= \boldsymbol{b}  \\
\boldsymbol{A}^{-1} \boldsymbol{Ax} &= \boldsymbol{A}^{-1} \boldsymbol{b} \\
\boldsymbol{Ix} &= \boldsymbol{A}^{-1} \boldsymbol{b} \\
\boldsymbol{x} &= \boldsymbol{A}^{-1} \boldsymbol{b} \\
\end{aligned}
$$

$\boldsymbol{A}^{-1}\boldsymbol{b}$ always yields a single vector, which is the unique solution to the equation. Our equation 

$$
\left( \boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T} \right) \boldsymbol{\rho_{\mu_{0}}^{\pi}} = \boldsymbol{\mu_{0} }
$$

takes on this exact form, such that

$$
\boldsymbol{\rho_{\mu_{0}}^{\pi}} =  \left( \boldsymbol{I} - \gamma {\boldsymbol{P}^{\pi}}^{T} \right)^{-1} \boldsymbol{\mu_{0} }
$$

is the unique solution to $\rho_{\mu_{0}}^{\pi}$.

### 1.5
Using the definition of $\nu_{\mu_{0}}^{\pi}$ from [1.2], we can rearrange to solve for $\pi$, then substitute in the definition of $\rho_{\mu_{0}}^{\pi}(s)$ from [1.2]:

$$
\begin{aligned}
\nu_{\mu_{0}}^{\pi}(s,a) &= \rho_{\mu_{0}}^{\pi}(s) \pi(a|s) \\
\pi(a|s)  &= \frac{\nu_{\mu_{0}}^{\pi}(s,a) }{\rho_{\mu_{0}}^{\pi}(s)} \\
\pi(a|s)  &= \frac{\nu_{\mu_{0}}^{\pi}(s,a) }{\sum_{a' \in \mathcal{A}} \nu_{\mu_{0}}^{\pi}(s,a')} \\
\end{aligned}
$$

This is the first part of the piecewise equation.

#### TODO: is there more to do here?

### 1.6
It is given that

$$
V^{\pi}(\mu) = \mathbb{E}_{S \sim \mu}\left[ V^{\pi}(S) \right]  = \sum_{s \in \mathcal{S}} \mu(s) V^{\pi}(s)
$$

So, substituting in $\mu_{0}$:

$$
\begin{aligned}
V^{\pi}(\mu_{0}) &= \mathbb{E}_{S \sim \mu_{0}}\left[ V^{\pi}(S) \right]  = \sum_{s \in \mathcal{S}} \mu_{0}(s) V^{\pi}(s) \\
V^{\pi}(\mu_{0}) &= \sum_{s \in \mathcal{S}} \mu_{0}(s) \mathbb{E} \left[  \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right] \\
V^{\pi}(\mu_{0}) &= \sum_{s \in \mathcal{S}} \mathbb{P}(s_{0} = s) \sum_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} \left[ \mathbb{P}(s_{0} = s | s_{0} = s) \mathbb{P}(A_{0} = a | s_{0} = s) \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) \right] \\
V^{\pi}(\mu_{0}) &= \sum_{s \in \mathcal{S}} \mathbb{P}(s_{0} = s) \sum_{a \in \mathcal{A}} \sum_{s \in \mathcal{S}} \left[ 1 \cdot \pi(a|s) \sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right] \\
\end{aligned}
$$

$$
v^{\pi}(s) = \mathbb{E} \left[
\sum_{t=0}^{\infty} \gamma^t R(s_t, a_t) | s_0 = s \right], \quad \forall s \in
\mathcal{S}
$$
