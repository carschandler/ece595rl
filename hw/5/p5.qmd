## Problem 5

### 5.1

We can represent the reward function and trajectories as data structures and
compute the discounted return for each sub-trajectory in Python:

```{python}
{{< include p5_1.py >}}
```

*Note: we also calculate the human preference probabilities at the end of this
code, used in the next two parts.*

The discounted returns for each sub-trajectory are as follows:

$$
\begin{aligned}
`{python} return_latex`
\end{aligned}
$$

### 5.2

The values are calculated at the end of the code block in [5.1] according to the
following equation:

$$
\mathbb{P}[\tau_1 > \tau_2] = \frac{ \exp \left( G(\tau_1) \right) }{ \exp\left( G(\tau_1) \right) + \exp\left( G(\tau_2) \right) }
$$

where $r_{\tau_i}$ is the cumulative discounted return for $\tau_i$ as
calculated in [5.1].

According to $\hat{R}$, the probability the human chooses $\tau_1$ over $\tau_2$
is:

$$
\mathbb{P}[\tau_1 > \tau_2] = \frac{ \exp \left( `{python} Markdown(f"{r1:.4f}")` \right) }{ \exp\left( `{python} Markdown(f"{r1:.4f}")` \right) + \exp\left( `{python} Markdown(f"{r2:.4f}")`  \right) } = `{python} Markdown(f"{pref_1_over_2:.5f}")` 
$$

### 5.3

Using the same formula, the probability the human chooses $\tau_1$ over $\tau_3$
is:

$$
\mathbb{P}[\tau_1 > \tau_3] = \frac{ \exp \left( `{python} Markdown(f"{r1:.4f}")` \right) }{ \exp\left( `{python} Markdown(f"{r1:.4f}")` \right) + \exp\left( `{python} Markdown(f"{r3:.4f}")`  \right) } = `{python} Markdown(f"{pref_1_over_3:.5f}")` 
$$

### 5.4

The MLE loss is calculated by summing log-probability terms for each point in
the dataset.

We can use probability values calculated in the previous parts to
fill in these variables. We calculate the loss in Python:

```{python}
{{< include p5_4.py >}}
```

The MLE loss according to $\hat{R}$ is $`{python} Markdown(f"{mle_loss:.4f}")`$.

### 5.5

If the human picks $\tau_2$ over $\tau_1$, then this is switching to a
trajectory with a lower return than in [5.4]. If it picks $\tau_3$ over
$\tau_1$, this is the same choice made in [5.4], so this choice has no effect on
the change in loss.

If switching from $\tau_2$ to $\tau_1$ means picking a trajectory with a lower
return than in [5.4], then this means **the loss will increase**. This can be
explained intuitively by the fact that choosing a worse return should increase
the loss value. Mathematically, the probability of choosing the lower return
will be smaller, which will cause the log-probability to decrease, causing the
overall loss to increase since it negates this term.
